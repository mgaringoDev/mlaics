---
title:  04 - Special applications Face recognition and Neural style transfer
author:     Mario Garingo
keywords: deepLearningSpecialization, cnn
summary: Discover how CNNs can be applied to multiple fields, including art generation and face recognition. Implement your own algorithm to generate art and recognize faces...
category: coursera
type: notes
sidebar: coursera_sidebar
---

## What is face recognition

- `liveness detection`

###  Face Verification vs Face Recognition

__Verification__
- input image, name/ID (1:1)
- output whether the input image is that of the claimed person

__Recognition__
- has a database of K people (1:K)
- get an input image
- output ID if the image is any of the K people (or `not recognized`)
- We can use a face verification system to make a face recognition system. The accuracy of the verification system has to be high (around 99.9% or more) to be use accurately within a recognition system because the recognition system accuracy will be less than the verification system given K persons. 
	- you basically need to solve a `one-shot-learning` problem


## One Shot Learning
- recognize a person with just one single image or given just one example of that person's face
	- traditionally NN don't work well when you only have one sample
		- you may only have one picture of that employee
	- this section will try and solve this problem

- `naive` approach
	- create a CNN with input images of the people
	- using softmax output labels of each person plus not-recognize
	- what happens if a new person comes in .... do you have to retrain the system ... this is not a good approach

- `similarity` function approach
	- d(img1, img2) =  degree of difference between images
		- if d()<= tau # `same` tau is a hyper parameter
		- if d() > tau # `different`

- make the CNN learn this similarity function such it helps us solving the one shot learning. Also its robust to new inputs.

## Siamese Network

- We will implement the similarity function using a type of NNs called Siamease Network in which we can pass multiple inputs to the two or more networks with the same architecture and parameters.

- Siamese network architecture are as the following:
  - We make 2 identical conv nets which encodes an input image into a vector. In the above image the vector shape is (128, )
  - The loss function will be `d(x1, x2) = || f(x1) - f(x2) ||^2`
  - If `X1`, `X2` are the same person, we want d to be low. If they are different persons, we want d to be high.

- > So what you want to do is really train the neural network so that the encoding that it computes results in a function d that tells you when two pictures are of the same person.

- parameters of the convNet define an encoding of the image to a 128 vector
$$ f(x^i)$$

- therefore you want to learn parameters so that:
	- if $$x^i$$ $$x^j$$ are the __same__ person then the loss function should be __small__
	- if $$x^i$$ $$x^j$$ are the __different__ person then the loss function should be __large__

	- > And what you can do is use back propagation and vary all those parameters in order to make sure these conditions are satisfied., marked from 4 minutes 20 seconds till 4 minutes 29 secondsAnd what you can do is use back propagation and vary all those parameters in order to make sure these conditions are satisfied.

- [[Taigman et. al., 2014. DeepFace closing the gap to human level performance]](https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Taigman_DeepFace_Closing_the_2014_CVPR_paper.html)

- The `triplet` loss function is the objective function we want to find 

## Triplet Loss

- there are 3 types of inputs
	- __anchor__ `A`: image of the person
	- __positive__ `P`: image that you are trying to positively identify
	- __negative__ `N`: image that is not the anchor image

### Learning Objective
![](https://drive.google.com/uc?id=1sXZmCOdHhTFfwv4rpr5MfMg1RBlekHNa)
- Formally we want:
  - Positive distance to be less than negative distance
  - `||f(A) - f(P)||^2  <= ||f(A) - f(N)||^2`
  - Then
  - `||f(A) - f(P)||^2  - ||f(A) - f(N)||^2 <= 0`

- there are trivial solution to this and that is when the first and second term is 0
	- the NN can learn these solutions and we want it to not give us this solution
- to fix this we add a margin called `alpha`
 	- `||f(A) - f(P)||^2  - ||f(A) - f(N)||^2 <= -alpha`
    - Alpha is a small number. Sometimes its called the margin.
  	- Then
  		- `||f(A) - f(P)||^2  - ||f(A) - f(N)||^2 + alpha <= 0`
  	- what the margin does is that it increases the distance between the anchor positive and anchor negative pairs away from each other.
  		- it does this by either increasing or decreasing the pairs such that the distance is large
  		- in the example if `alpha` is 0.2 then if `d(A,P)` = 0.5 then `d(A,N)` needs to be 0.7 or higher

### Loss Function
![](https://drive.google.com/uc?id=1dwLdM43xThYtPaD504I8tGsN6TI-MWie)
- given 3 images `A`,`P`,`N`
- `L(A, P, N) = max (||f(A) - f(P)||^2  - ||f(A) - f(N)||^2 + alpha , 0)`
	- > So, the effect of taking the max here is that, so long as this is __less than zero__, then the __loss is zero, because the max is something less than equal to zero__, when zero is going to be zero.
	- > But if on the other hand, if this is __greater than zero__, then if you take the max, the __max we end up selecting__, this thing I've underlined in green, and so you would have a __positive loss__.
- `J = Sum(L(A[i], P[i], N[i]) , i)` for all triplets of images.
	- > But of course after training, if you're applying this, but of course after having trained the system, you can then apply it to your one shot learning problem where for your face recognition system, maybe you have only a single picture of someone you might be trying to recognize.
	- > But for your training set, you do need to make sure you have multiple images of the same person at least for some people in your training set so that you can have pairs of anchor and positive images.


### Choosing Triplets
![](https://drive.google.com/uc?id=1UgB3ofyDn6fdpmxaOKfdA9MxrP1WUDCw)
- > If you choose your triplets randomly, then too many triplets would be really easy, and so, gradient descent won't do anything because your neural network will just get them right, pretty much all the time.

- > And it's only by using hard triplets that the gradient descent procedure has to do some work to try to push these quantities further away from those quantities.

- Choosing the triplets A, P, N:
  - During training if A, P, N are chosen randomly (Subjet to A and P are the same and A and N aren't the same) then one of the problems this constrain is easily satisfied 
    - `d(A, P) + alpha <= d (A, N)` 
    - So the NN wont learn much
  - What we want to do is choose triplets that are **hard** to train on.
    - So for all the triplets we want this to be satisfied:
    - `d(A, P) + alpha <= d (A, N)`
    - This can be achieved by for example same poses

- Details are in this paper [[Schroff et al.,2015, FaceNet: A unified embedding for face recognition and clustering]](https://arxiv.org/abs/1503.03832)

- Commercial recognition systems are trained on a large datasets like 10-100 million images.
- There are a lot of pretrained models and parameters online for face recognition.
	- you should use download pre-trained model

## Face Verification and Binary Classification 
- Triplet loss is one way to learn the parameters of a conv net for face recognition there's another way to learn these parameters as a straight binary classification problem.
- ![](https://drive.google.com/uc?id=1gBVhaTfYPR1lVexZa8jX7kKi6Z1CC6aI)
- Learning the similarity function another way:  
  - The final layer is a sigmoid layer.
  - `Y' = wi * Sigmoid ( f(x(i)) - f(x(j)) ) + b` where the subtraction is the Manhattan distance between f(x(i)) and f(x(j))
  - you can have a logistic regression unit in the sigmoid by adding a `w` and `b`
  	- train weighting in order to predict if two images are different
  - Some other similarities can be Euclidean and Ki square similarity.
  - The NN here is Siamese means the top and bottom convs has the same parameters.
- The paper for this work: [[Taigman et. al., 2014. DeepFace closing the gap to human level performance]](https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Taigman_DeepFace_Closing_the_2014_CVPR_paper.html)
- the input is a pair of images and the output y is 1,0 depending whether they are similar or not
	- each convNet has the same or similar learning parameters and weights like in the Siamese convnet

- A good performance/deployment trick:
  - Pre-compute all the images that you are using as a comparison to the vector f(x(j))
  - When a new image that needs to be compared, get its vector f(x(i)) then put it with all the pre computed vectors and pass it to the sigmoid function.
- This version works quite as well as the triplet loss function.
- Available implementations for face recognition using deep learning includes:
  - [Openface](https://cmusatyalab.github.io/openface/)
  - [FaceNet](https://github.com/davidsandberg/facenet)
  - [DeepFace](https://github.com/RiweiChen/DeepFace)


## What is Neural Style Transfer?

- Neural style transfer is one of the application of Conv nets.
- Neural style transfer takes a content image `C` and a style image `S` and generates the content image `G` with the style of style image.
- ![](https://drive.google.com/uc?id=1R3sa7G3v-A0LoFrEjaCtSvzL02TY7DQD)
- In order to implement this you need to look at the features extracted by the Conv net at the shallower and deeper layers.
- It uses a previously trained convolutional network like VGG, and builds on top of that. The idea of using a network trained on a different task and applying it to a new task is called transfer learning.
- >  In order to implement Neural Style Transfer, you need to look at the features extracted by ConvNet at various layers, the shallow and the deeper layers of a ConvNet., marked from 1 minute 37 seconds till 1 minute 47 secondsIn order to implement Neural Style Transfer, you need to look at the features extracted by ConvNet at various layers, the shallow and the deeper layers of a ConvNet. Before diving into how you can implement a Neural Style Transfer, what I want to do in the next video is try to give you better intuition about whether all these layers of a ConvNet really computing.

## What are deep ConvNets learning?

### Visualizing what a DNN is learning
![](https://drive.google.com/uc?id=1xOhm7K9iAfNL-SJTqgsLePQOgZGyJ4fN)
#### Earlier Layer
- earlier units are exposed to smaller section of the input image
- pick a unit in layer 1. Find he nine image patches that maximizes the unit's activation, and repeat for other units
	- > And so if you visualize, if you plot what activated unit's activation, it makes makes sense to plot just a small image patches, because all of the image that that particular unit sees.
	- ![](https://drive.google.com/uc?id=1OkqHap8nB9i7_rhdXF54M0VAClzY1Qs9)
	- > So this gives you a sense that, units, train hidden units in layer 1, they're often looking for relatively simple features such as edge or a particular shade of color.
- [[Zeiler and Fergus., 2013, Visualizing and understanding convolutional networks]](https://arxiv.org/abs/1311.2901)


#### Deeper Layer
- deeper units are exposed to larger section of the input image patches
- more complex shapes images show up as you go deeper and deeper

- A good explanation on how to get **receptive field** given a layer:
  - From [A guide to receptive field arithmetic for Convolutional Neural Networks](https://medium.com/@nikasa1889/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807)

## Cost Function for Neural Style Transfer
![](https://drive.google.com/uc?id=1Z_qCRyXHE1XxDJ0Wi06eaZ5Cy4-ElLHr)
- We will define a cost function for the generated image that measures how good it is.
- Give a content image C, a style image S, and a generated image G:
  - `J(G) = alpha * J(C,G) + beta * J(S,G)`
  - `J(C, G)` measures how similar is the generated image to the Content image.
  - `J(S, G)` measures how similar is the generated image to the Style image.
  - alpha and beta are relative weighting to the similarity and these are hyperparameters.

![](https://drive.google.com/uc?id=1ClIhvsxwZSpNHA3pCzYr4EIaMZewcb6m)
- Find the generated image G:
  1. Initiate G randomly
     - For example G: 100 X 100 X 3
  2. Use gradient descent to minimize `J(G)`
     - `G = G - dG`  We compute the gradient image and use gradient decent to minimize the cost function.

## Content Cost Function

- Say you use hidden layer `l` to compute content cost. 
  - If we choose `l` to be small (like layer 1), we will force the network to get similar output to the original content image.
  - iIf we choose `l` to be small (like layer 10), then it's just asking, "Well, if there is a dog in your content image,
  - In practice `l` is not too shallow and not too deep but in the middle.
- Use pre-trained ConvNet. (E.g., VGG network)
- Let `a(c)[l]` and `a(G)[l]` be the activation of layer `l` on the images.
	- these are unrolled into vectors
- If `a(c)[l]` and `a(G)[l]` are similar then they will have the same content
  - `J(C, G) at a layer l = 1/2 || a(c)[l] - a(G)[l] ||^2`
  - 1/2 is a normalization parameter

## Style Cost Function

![](https://drive.google.com/uc?id=1BLnn6fU11w27p8ZJJY_mpbHFWskxgBJb)
- Meaning of the ***style*** of an image:
  - Say you are using layer l's activation to measure ***style***.
  - Define style as correlation between **activations** across **channels**. 
    - How correlate is the orange channel with the yellow channel?
    - Correlated means if a value appeared in a specific channel a specific value will appear too (Depends on each other).
    - Uncorrelated means if a value appeared in a specific channel doesn't mean that another value will appear (Not depend on each other)
  - The correlation tells you how a components might occur or not occur together in the same image.
- The correlation of style image channels should appear in the generated image channels.
- > And so the correlation tells you which of these high level texture components tend to occur or not occur together in part of an image and that's the degree of correlation that gives you one way of measuring how often these different high level features, such as vertical texture or this orange tint occur or don't occur together

![](https://drive.google.com/uc?id=1TRF-pbElXt6-bluTdPmrNw6vHc67fFIy)
- Style matrix (Gram matrix):
  - Let `a(l)[i, j, k]` be the activation at l with `(i=H, j=W, k=C)`
  - Also `G(l)(s)` is matrix of shape `nc(l) x nc(l)`
    - We call this matrix style matrix or Gram matrix.
    - In this matrix each cell will tell us how correlated is a channel to another channel.
  - To populate the matrix we use these equations to compute style matrix of the style image and the generated image.    
    - As it appears its the sum of the multiplication of each member in the matrix.
  - now you have a matrix that represents the style of the image and the style of th image G
  	- G is used because in linear algeo this is called `gram matrix`

- To compute gram matrix efficiently:
  - Reshape activation from H X W X C to HW X C
  - Name the reshaped activation F.
  - `G[l] = F * F.T`

- > And, finally, it turns out that you get more visually pleasing results if you use the style cost function from multiple different layers.
- Finally the cost function will be as following:
  - `J(S, G) at layer l = (1/ 2 * H * W * C) || G(l)(s) - G(l)(G) ||`
- And if you have used it from some layers
  - `J(S, G) = Sum (lamda[l]*J(S, G)[l], for all layers)`
  - lamda enables you to combine lower and higher level layers
![](https://drive.google.com/uc?id=1BGNFCVvReh2bwg7BRskasYh2SLFEJ6yn)

- Steps to be made if you want to create a tensorflow model for neural style transfer:
  1. Create an Interactive Session.
  2. Load the content image.
  3. Load the style image
  4. Randomly initialize the image to be generated
  5. Load the VGG16 model
  6. Build the TensorFlow graph:
     - Run the content image through the VGG16 model and compute the content cost
     - Run the style image through the VGG16 model and compute the style cost
     - Compute the total cost
     - Define the optimizer and the learning rate
  7. Initialize the TensorFlow graph and run it for a large number of iterations, updating the generated image at every step.

## 1D and 3D Generalizations

- So far we have used the Conv nets for images which are 2D.
- Conv nets can work with 1D and 3D data as well.

### 1D
![](https://drive.google.com/uc?id=1GMzsFV_tQPHe5oIEa9ve3kIEXH8uRM3i)
- An example of 1D convolution:
  - Input shape (14, 1)
  - Applying 16 filters with F = 5 , S = 1
  - Output shape will be 10 X 16
  - Applying 32 filters with F = 5, S = 1
  - Output shape will be 6 X 32
- The general equation `(N - F)/S + 1` can be applied here but here it gives a vector rather than a 2D matrix.
- 1D data comes from a lot of resources such as waves, sounds, heartbeat signals. 
- In most of the applications that uses __1D data we use Recurrent Neural Network RNN.__

### 3D
![](https://drive.google.com/uc?id=1v5FRSJr77H8rgrxT8nt4TXCZ1DipC8mW)
- 3D data also are available in some applications like CT scan
	- generalize CNN by applying `n` number of 3D filters 

- another example of 3D inputs is movies and different slice in the movies and you are trying to detect movement or different actions in the movie


## Keras

- Keras is a high-level neural networks API (programming framework), written in Python and capable of running on top of several lower-level frameworks including TensorFlow, Theano, and CNTK.
- Keras was developed to enable deep learning engineers to build and experiment with different models very quickly.
- Just as TensorFlow is a higher-level framework than Python, Keras is an even higher-level framework and provides additional abstractions.
- Keras will work fine for many common models.
- Layers in Keras:
  - Dense (Fully connected layers).
    - A linear function followed by a non linear function.
  - Convolutional layer.
  - Pooling layer.
  - Normalisation layer.
    - A batch normalization layer.
  - Flatten layer
    - Flatten a matrix into vector.
  - Activation layer
    - Different activations include: relu, tanh, sigmoid, and softmax.
- To train and test a model in Keras there are four steps:
  1. Create the model.
  2. Compile the model by calling `model.compile(optimizer = "...", loss = "...", metrics = ["accuracy"])`
  3. Train the model on train data by calling `model.fit(x = ..., y = ..., epochs = ..., batch_size = ...)`
     - You can add a validation set while training too.
  4. Test the model on test data by calling `model.evaluate(x = ..., y = ...)`
- Summarize of step in Keras: Create->Compile->Fit/Train->Evaluate/Test
- `Model.summary()` gives a lot of useful informations regarding your model including each layers inputs, outputs, and number of parameters at each layer.
- To choose the Keras backend you should go to `$HOME/.keras/keras.json` and change the file to the desired backend like Theano or Tensorflow or whatever backend you want.
- After you create the model you can run it in a tensorflow session without compiling, training, and testing capabilities.
- You can save your model with `model_save` and load your model using `model_load ` This will save your whole trained model to disk with the trained weights.

