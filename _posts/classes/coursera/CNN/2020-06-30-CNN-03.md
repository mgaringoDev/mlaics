---
title:  03 - Object detection
author:     Mario Garingo
keywords: deepLearningSpecialization, cnn
summary: Learn how to apply your knowledge of CNNs to one of the toughest but hottest field of computer vision. Object detection....
category: coursera
type: notes
sidebar: coursera_sidebar
---

## Object Localization

![](objectDetectClassification)
- **Image Classification**: 
    - Classify an image to a specific class. __The whole image represents one class.__ We don't want to know exactly where are the object. Usually only one object is presented.    
- **Classification with localization**:
    - Given an image we want to learn the class of the image and where are the class location in the image. __We need to detect a class and a bounding box of where that object is.__ Usually only one object is presented.  

- **Object detection**:
    - Given an image we want to detect all the object in the image that belong to a specific classes and give their location. __An image can contain more than one object with different classes.__
- things that you've learned in the localization algorithm can be transfered to the detection algorithm

- **Semantic Segmentation**:
    - We want to Label each pixel in the image with a category label. Semantic Segmentation Don't differentiate instances, only care about pixels. It detects no objects just pixels.
    - If there are two objects of the same class is intersected, we won't be able to separate them.    
- **Instance Segmentation**
    - This is like the full problem. Rather than we want to predict the bounding box, we want to know which pixel label but also distinguish them.    


- To make image classification we use a Conv Net with a Softmax attached to the end of it.

- To make classification with localization we use a Conv Net with a softmax attached to the end of it and a four numbers `bx`, `by`, `bh`, and `bw` to tell you the location of the class in the image. The dataset should contain this four numbers with the class too.
	- `bx` and `by` are the center of the bounding box
	- `bh` and `bw` are the width and hight of the bounding box


### Defining x and y
- `x` will be your input image
- `y` will be a vector that is comprised of:
```
 Y = [
  		Pc				# Probability of an object is presented
  		bx				# Bounding box
  		by				# Bounding box
  		bh				# Bounding box
  		bw				# Bounding box
  		c1				# pedestrian (class 1)
  		c2        		# car (class 2)
  		c3        		# motorcycle (class 3)
    ]
```
![](definingXY)

### Defining the loss 
- The loss function for the Y we have created (Example of the square error):

  - ```
    L(y',y) = {
      			(y1'-y1)^2 + (y2'-y2)^2 + ... (y8'-y8)^2           if y1 = 1
      			(y1'-y1)^2						if y1 = 0
    		}
    ```

  - `y1` is the first value of the `y` vector.  When it is `1` that means an object is present, so you would want to obtain the square error.  But if there is no object you don't really care about anything but the first entry.  You penalize whether the classification was done correctly if an object was present.

  - In practice we use logistic regression or loglikihood loss for `pc`, log likely hood loss for classes, and squared error for the bounding box.

## Landmark Detection

![](landmarkDetectionExamples)
- In some of the computer vision problems you will need to output some points. That is called **landmark detection**.

- For example, if you are working in a face recognition problem you might want some points on the face like corners of the eyes, corners of the mouth, and corners of the nose and so on. This can help in a lot of application like detecting the pose of the face.

- Y shape for the face recognition problem that needs to output 64 landmarks:

  ```
    Y = [
        THereIsAface        # Probability of face is presented 0 or 1
        l1x,
        l1y,
        ....,
        l64x,
        l64y
  ]
  ```

- the pipeline would be
	- x = the image
	- convNet
	- y = the Y vector above with 64 landmarks (129 output)
	- these are the key building blocks
		- recognizing emotions
		- face structure
		- AR applications

- Another application is when you need to get the skeleton of the person using different landmarks/points in the person which helps in some applications.

- labels must be consistent across labeled data across different images


## Object Detection

- We will use a Conv net to solve the object detection problem using a technique called the sliding windows detection algorithm.
- For example lets say we are working on Car object detection.
- The first thing, we will train a Conv net on cropped car images and non car images.

![](slidingWindowExamples)

- After we finish training of this Conv net we will then use it with the sliding windows technique.
- Sliding windows detection algorithm:
  1. Decide a rectangle size.
  2. Split your image into rectangles of the size you picked. Each region should be covered. You can use some strides.
  3. For each rectangle feed the image into the Conv net and decide if its a car or not.
  4. Pick larger/smaller rectangles and repeat the process from 2 to 3.
  5. Store the rectangles that contains the cars.
  6. If two or more rectangles intersects choose the rectangle with the best accuracy.

- so long as you have the object in the image then a square bounding box can detect an object

- the downside is the computational costs
	- depending on the stride length selected the algorithm might need to go through the image longer or shorter 
- in the past this was a viable method because we trained shallower algorithms but now we have convNets and the time it takes for a calculation to be made is longer
	- unless you have a small stride then you may miss classify the image

- In the era of machine learning before deep learning, people used a hand crafted linear classifiers that classifies the object and then use the sliding window technique. The linear classier make it a cheap computation. But in the deep learning era that is so  computational expensive due to the complexity of the deep learning model.
- To solve this problem, we can implement the sliding windows with a ***Convolutional approach***.

## Convolutional Implementation of Sliding Windows

![](fc2conv)
- As you can see in the above image, we turned the FC layer into a Conv layer using a convolution with the width and height of the filter is the same as the width and height of the input.

### Convolution implementation of sliding windows

![](slidingWindowExample)

- First lets consider that the Conv net you trained is like this (No FC all is conv layers):  
  - Say now we have a 16 x 16 x 3 image that we need to apply the sliding windows in. By the normal implementation that have been mentioned in the section before this, we would run this Conv net four times each rectangle size will be 16 x 16.
  - Simply we have feed the image into the same Conv net we have trained.
  - The left cell of the result "The blue one" will represent the the first sliding window of the normal implementation. The other cells will represent the others.
  - Its more efficient because it now shares the computations of the four times needed.

- > So what this process does, what this convolution implementation does is, instead of forcing you to run four propagation on four subsets of the input image independently, Instead, it combines all four into one form of computation and shares a lot of the computation in the regions of image that are common

- the max pool stride indicate the stride of the sliding window inside the original image

- The weakness of the algorithm is that the position of the rectangle wont be so accurate. Maybe none of the rectangles is exactly on the object you want to recognize.

- [[Sermanet et al., 2014, OverFeat: Integrated recognition, localization and detection using convolutional networks]](https://arxiv.org/abs/1312.6229)

## Bounding Box Predictions


- A better algorithm than the one described in the last section is the [YOLO algorithm](https://arxiv.org/abs/1506.02640).

- YOLO stands for *you only look once* and was developed back in 2015.

- Yolo Algorithm:

  ![](yoloAlgo)

  1. Lets say we have an image of 100 X 100
  2. Place a  3 x 3 grid on the image. For more smother results you should use 19 x 19 for the 100 x 100
  3. Apply the classification and localization algorithm we discussed in a previous section to each section of the grid. `bx` and `by` will represent the center point of the object in each grid and will be relative to the box so the range is between 0 and 1 while `bh` and `bw` will represent the height and width of the object which can be greater than 1.0 but still a floating point value.
  4. Do everything at once with the convolution sliding window. If Y shape is 1 x 8 as we discussed before then the output of the 100 x 100 image should be 3 x 3 x 8 which corresponds to 9 cell results.
  5. Merging the results using predicted localization mid point.

- We have a problem if we have found more than one object in one grid box.
- the midpoint of the object should only be in one grib box so as to not classify into two grib box

- One of the best advantages that makes the YOLO algorithm popular is that it has a great speed and a Conv net implementation.
  -  > it outputs the bounding balls coordinates explicitly. This allows in your network to output bounding boxes of any aspect ratio, as well as, output much more precise coordinates that aren't just dictated by the stripe size of your sliding windows classifier.
  - > And second, this is a convolutional implementation and you're not implementing this algorithm nine times on the 3 by 3 grid or if you're using a 19 by 19 grid.19 squared is 361.


- How is YOLO different from other Object detectors?  YOLO uses a single CNN
  - network for both classification and localizing the object using bounding boxes.
  - very fast and can run on real time application 

### Specifying the bounding boxes

![](specifyTheBoundingBoxes)

```
 Y = [
  		Pc				# Probability of an object is presented
  		bx				# specified relative of the grid cell 
  		by				# between 0 - 1
  		bh				# specified relative of the grid cell 
  		bw				# can be > 1 because it can go onto the next grid cell
  		c1				# pedestrian (class 1)
  		c2        		# car (class 2)
  		c3        		# motorcycle (class 3)
    ]
```

- there are some parameterization of the bounding boxes such as:
	- Although, there are some more complicated parameterizations involving sigmoid functions to make sure this is between 0 and 1.
	- And using an explanation parameterization to make sure that these are non-negative, since 0.9, 0.5, this has to be greater or equal to zero

## Intersection Over Union

- Intersection Over Union is a function used to evaluate the object detection algorithm.
- It computes size of intersection and divide it by the union. More generally, *IoU* *is a measure of the overlap between two bounding boxes*.
- For example:
  - ![](iou)
  - The red is the labeled output and the purple is the predicted output.
  - To compute Intersection Over Union we first compute the union area of the two rectangles which is "the first rectangle + second rectangle" Then compute the intersection area between these two rectangles.
  - Finally `IOU = intersection area / Union area`
- If `IOU >=0.5` then its good. The best answer will be 1.
	- closer to 1 the more stringent you are for classifying the bounding boxes
- The higher the IOU the better is the accuracy.

## Non-max Suppression

![](nonMaxSuppression)
- And non-max means that you're going to output your maximal probabilities classifications but suppress the close-by ones that are non-maximal.

- For example:
  - ![](nonMaxSupExample)
  - Each car has two or more detections with different probabilities. This came from some of the grids that thinks that this is the center point of the object.
- Non-max suppression algorithm:
  1. Lets assume that we are targeting one class as an output class.
  2. Y shape should be `[Pc, bx, by, bh, hw]` Where Pc is the probability if that object occurs.
  3. Discard all boxes with `Pc < 0.6`  
  4. While there are any remaining boxes:
     1. Pick the box with the largest Pc Output that as a prediction.
     2. Discard any remaining box with `IoU > 0.5` with that box output in the previous step i.e any box with high overlap(greater than overlap threshold of 0.5).
- If there are multiple classes/object types `c` you want to detect, you should run the Non-max suppression `c` times, once for every output class. [look at the program exercise] 

### Anchor Boxes

![](objectOverlap)
- There are anchor boxes of different length and width.  For each of the anchor boxes they have an associated Y label (in the example you have the 8 that we talk about in the previous lectures). You concat them vertically for each of the anchor boxes, and you encode the correct label for the specific anchorbox that closely resembles the object in question

![](anchorBox)
- So Previously, each object in training image is assigned to grid cell that contains that object's midpoint.
- With two anchor boxes, Each object in training image is assigned to grid cell that contains object's midpoint and anchor box for the grid cell with __highest IoU__. You have to check where your object should be based on its rectangle closest to which anchor box.

- YOLO fails if:
	- > two anchor boxes but three objects in the same grid cell? That's one case that this algorithm doesn't handle well.
	- > hard code  a tie breaker or default way of tie breaker

![](achorBoxExample)
- a grid box that has two object with different bounding box shapes will have labeled and associated with it on the `y` vector which corresponds to the bounding box.  In the example yellow was anchor box 1 and green was anchor box 2
	- the pedestrian is closer to anchor box 1
	- the car is closer to anchor box 2
	- they get encoded separately in the same grid 
- on the other hand if there is only car then the `y` vector will only have true values in the second anchor box while the first anchor box will have 0 on its first set element and `?` on the remainder

- __how do you choose the anchor boxes__ 
	- people used to just choose them by hand. Maybe five or ten anchor box shapes that spans a variety  of shapes that cover the types of objects you seem to detect frequently.
	- You may also use a k-means algorithm on your dataset to specify that.
- Anchor boxes allows your algorithm to specialize, means in our case to easily detect wider images or taller ones.

## YOLO Algorithm

### Training
![](yoloTraining)
- Suppose we need to do object detection for our autonomous driver system.It needs to identify three classes:

  1. Pedestrian (Walks on ground).
  2. Car.
  3. Motorcycle.

- We decided to choose two anchor boxes, a taller one and a wide one.

  - Like we said in practice they use five or more anchor boxes hand made or generated using k-means.

- Our labeled Y shape will be `[Ny, HeightOfGrid, WidthOfGrid, 16]`, where Ny is number of instances and each row (of size 16) is as follows:

  - `[Pc, bx, by, bh, bw, c1, c2, c3, Pc, bx, by, bh, bw, c1, c2, c3]`
  - __output Volume__ = `3 x 3 x 2 x 8`
  - (grid size) x (# of anchors) x (PC + 4 bounding + # of classes)
  - in practice we would use 19 grid boxes and about 5 anchor boxes so it would be 19x19x40

### Predictions
- To make predictions, run the Conv net on an image and run Non-max suppression algorithm for each class you have in our case there are 3 classes.

  - You could get something like that:
    - ![](yoloEx1.png)
    - Total number of generated boxes are grid_width * grid_height * no_of_anchors = 3 x 3 x 2
  - By removing the low probability predictions you should have:
    - ![](yoloEx2.png)
  - Then get the best probability followed by the IOU filtering:
  	- for each class (independently) use non-max suppression to generate final prediction
    - ![](yoloEx3.png)

- YOLO are not good at detecting smaller object.

- [YOLO9000 Better, faster, stronger](https://arxiv.org/abs/1612.08242)

### Some YOLO implementation
- You can find implementations for YOLO here:

  - https://github.com/allanzelener/YAD2K
  - https://github.com/thtrieu/darkflow
  - https://pjreddie.com/darknet/yolo/
