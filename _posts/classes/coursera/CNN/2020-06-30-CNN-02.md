---
title:  02 - Deep convolutional models
author:     Mario Garingo
keywords: deepLearningSpecialization, cnn
summary: Learn about the practical tricks and methods used in deep CNNs straight from the research papers. ...
category: coursera
type: notes
sidebar: coursera_sidebar
---

## Why look at case studies?

- We learned about Conv layer, pooling layer, and fully connected layers. It turns out that computer vision researchers spent the past few years on how to put these layers together.
- To get some intuitions you have to see the examples that has been made.
- Some neural networks architecture that works well in some tasks can also work well in other tasks.
- Here are some classical CNN networks:
  - **LeNet-5**
  	- 1980's
  - **AlexNet**
  - **VGG**
- The best CNN architecture that won the last ImageNet competition is called **ResNet** and it has 152 layers!
	- residual networks
	- shows interesting tricks and optimization techniques
- There are also an architecture called **Inception** that was made by Google that are very useful to learn and apply to your tasks.
- Reading and trying the mentioned models can boost you and give you a lot of ideas to solve your task. 

## Classic Networks

### LeNet-5

- ![](lenet5)
- goal was to recognize hand written images and gray scale
	- in the past they used `avg pool` but now you would use `max pool` 
	- in the past they also did not use padding so each conv layer the image decreases
	- in the past the output `y_hat` had 10 labels (0-9) digits but the modern version you would use a `softmax` to do this classification
- this is considered small parameters with 60K parameters
- as you go from left to right the n_H, n_W decrease but the number of channels/ filters increase
- `Conv ==> Pool ==> Conv ==> Pool ==> FC ==> FC ==> softmax` this type of arrangement is quite common.
- The activation function used in the paper was Sigmoid and Tanh. Modern implementation uses RELU in most of the cases.
- [[LeCun et al., 1998. Gradient-based learning applied to document recognition]](http://ieeexplore.ieee.org/document/726791/?reload=true)
	- only section II and III is relevant these days
- __note__:
	- And so to save on computation as well as some parameters, the original LeNet-5 had some crazy complicated way where different filters would look at different channels of the input block.  Play video for highlighted transcript with text, And so the paper talks about those details, but the more modern implementation wouldn't have that type of complexity these days., marked from 5 minutes 58 seconds till 6 minutes 7 secondsAnd so the paper talks about those details, but the more modern implementation wouldn't have that type of complexity these days.
	- And then one last thing that was done back then I guess but isn't really done right now is that the original LeNet-5 had a non-linearity after pooling, and I think it actually uses sigmoid non-linearity after the pooling layer.

### AlexNet

- ![](alexNet)
- The goal for the model was the ImageNet challenge which classifies images into 1000 classes
- had a lot of similarities to lenet but larger
	- 60 million parameters as compared to 60K
	- this uses the ReLU activation
- things to keep in mind when reading the paper:
	- multiple GPUs
	- local response normalization (LRN)
		- looks at one position and looks down the channel and normalize them
		- for each position in the 13/13 image you don't want any neuron with high activation
			- we have proven that this doesn't really do that much
- This paper convinced the computer vision researchers that deep learning is so important.
- [[Krizhevsky et al., 2012. ImageNet classification with deep convolutional neural networks]](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)

### VGG-16

- ![](vgg16)
- A modification for AlexNet.
- Instead of having a lot of hyperparameters lets have some simpler network.
- Focus on having only these blocks:
	- CONV = 3 X 3 filter, s = 1, same  
	- MAX-POOL = 2 X 2 , s = 2
	- this simplifies the architecture
- pooling layer reduces hight and width of the tensor.
- This network is large even by modern standards. It has around 138 million parameters.
	- Most of the parameters are in the fully connected layers.
	- It has a total memory of 96MB per image for only forward propagation!
	- Most memory are in the earlier layers.
- Number of filters increases from 64 to 128 to 256 to 512. 512 was made twice.
- Pooling was the only one who is responsible for shrinking the dimensions.
- as you go deeper and deeper the hight and width decrease but the number of filters increase
- There are another version called **VGG-19** which is a bigger version. But most people uses the VGG-16 instead of the VGG-19 because it does the same.
- VGG paper is attractive it tries to make some rules regarding using CNNs.
- [[Simonyan & Zisserman 2015. Very deep convolutional networks for large-scale image recognition]](https://arxiv.org/abs/1409.1556)

## Residual Networks (ResNets)

- Very, very deep NNs are difficult to train because of vanishing and exploding gradients problems, this method shows a method to combat that.

### Residual Block
- ResNets are built out of some Residual blocks.
- ![](resBlock)
- They add a shortcut/skip connection before the second activation.

### Residual Block Networks
- The authors of this block find that you can train a deeper NNs using stacking this block.
- taking these blocks and stacking them together to create a deeper network
	- the following shows 5 res block
	- ![](5ResBlock)
- [[He et al., 2015. Deep residual networks for image recognition]](https://arxiv.org/abs/1512.03385)
- ![](theoryVsRealityOfIncreaseLayers)
	- theory having a deeper network should help
	- all your optimization algorithm just has a much harder time training so error gets worse


## Why ResNets Work

- ![](scannedResidualNetworkNotes)
- Lets see some example that illustrates why resNet work.

  - We have a big NN as the following:

    - `X --> Big NN --> a[l]`

  - Lets add two layers to this network as a residual block:

    - `X --> Big NN --> a[l] --> Layer1 --> Layer2 --> a[l+2]`
    - And a`[l]` has a direct connection to `a[l+2]`

  - Suppose we are using RELU activations.

  - Then:

    - ```
      a[l+2] = g( z[l+2] + a[l] )
      	   = g( W[l+2] a[l+1] + b[l+2] + a[l] )
      ```

  - Then if we are using L2 regularization for example, `W[l+2]` will be zero. Lets say that `b[l+2]` will be zero too.

  - Then `a[l+2] = g( a[l] ) = a[l]` with no negative values.

  - This show that identity function is easy for a residual block to learn. And that why it can train deeper NNs.

  - Also that the two layers we added doesn't hurt the performance of big NN we made.

  - Hint: dimensions of z[l+2] and a[l] have to be the same in resNets. In case they have different dimensions what we put a matrix parameters (Which can be learned or fixed)

    - `a[l+2] = g( z[l+2] + ws * a[l] ) # The added Ws should make the dimensions equal`
    - ws also can be a zero padding to increase or fixed matrix

- > And what goes wrong in very deep plain nets in very deep network without this residual of the skip connections is that when you make the network deeper and deeper, it's actually very difficult for it to choose parameters that learn even the identity function which is why a lot of layers end up making your result worse rather than better.  And I think the main reason the residual network works is that it's so easy for these extra layers to learn the identity function that you're kind of guaranteed that it doesn't hurt performance and then a lot the time you maybe get lucky and then even helps performance.

- Lets take a look at ResNet on images.

  - Here are the architecture of **ResNet-34**:
  - ![](resNetExample)
  - All the 3x3 Conv are same Convs.
  - Keep it simple in design of the network.
  - spatial size /2 => # filters x2
  - No FC layers, No dropout is used.
  - Two main types of blocks are used in a ResNet, depending mainly on whether the input/output dimensions are same or different. You are going to implement both of them.
  - The dotted lines is the case when the dimensions are different. To solve then they down-sample the input by 2 and then pad zeros to match the two dimensions. There's another trick which is called bottleneck which we will explore later.

## Network in Network and 1 X 1 convolutions

- ![](1x1Filter)
- > So one way to think about the one by one convolution is that, it is basically having a fully connected neuron network, that applies to each of the 62 different positions.
- ![](shrinkNumChannels)  
- you can think of this as shrinking the depth of the layer.  Pooling shrinks the hight and width of the network but 1x1 filters with the number of filters being equal to the desired depth of the next layer

- Replace fully connected layers with 1 x 1 convolutions as Yann LeCun believes they are the same.

  - > In Convolutional Nets, there is no such thing as "fully-connected layers". There are only convolution layers with 1x1 convolution kernels and a full connection table. [Yann LeCun](https://www.facebook.com/yann.lecun/posts/10152820758292143) 

- [[Lin et al., 2013. Network in network]](https://arxiv.org/abs/1312.4400)

## Inception Network Motivation

- When you design a CNN you have to decide all the layers yourself. Will you pick a 3 x 3 Conv or 5 x 5 Conv or maybe a max pooling layer. You have so many choices.
 - [[Szegedy et al. 2014. Going deeper with convolutions]](https://arxiv.org/abs/1409.4842)
- ![](inceptionLayer)
- What **inception** tells us is, Why not use all of them at once?
	- pooling should have padding with stride of 1 to keep the dimensions the same
	- input some volume apply multiple types of filters and concats them and it learns all the parameters it wants as well as which filter to use in the end
	- The real problem is the computational cost
- ![](inceptionModule1x1)
- to combat this you can use 1x1 convolution first to reduce the number of channels, then apply a desired filter

## Inception network (GoogleNet)

- ![](inceptionModule)
- the above is just one layer
- ![](inceptionNN)
	- max pooling to change the dimension of hight and width of the volume going in
	- there are side branches, and what they do is just make a prediction
		- help to ensure that intermediate layers are obtaining the correct error
		- acting like a regularization as well
	- Some times a Max-Pool block is used before the inception module to reduce the dimensions of the inputs.
	- There are a 3 Sofmax branches at different positions to push the network toward its goal. and helps to ensure that the intermediate features are good enough to the network to learn and it turns out that softmax0 and sofmax1 gives regularization effect.
	- Since the development of the Inception module, the authors and the others have built another versions of this network. Like inception v2, v3, and v4. Also there is a network that has used the inception module and the ResNet together.
	- [[Szegedy et al., 2014, Going Deeper with Convolutions]](https://arxiv.org/abs/1409.4842)
- there are also some modification of inception modules and merging them with concepts such as skip or short circuit techniques.

## Using Open-Source Implementation

- We have learned a lot of NNs and ConvNets architectures.
- It turns out that a lot of these NN are difficult to replicated. because there are some details that may not presented on its papers. There are some other reasons like:
  - Learning decay.
  - Parameter tuning.
- If you see a research paper and you want to build over it, the first thing you should do is to look for an open source implementation for this paper.
- Some advantage of doing this is that you might download the network implementation along with its parameters/weights. The author might have used multiple GPUs and spent some weeks to reach this result and its right in front of you after you download it.

## Transfer Learning

- If you are using a specific NN architecture that has been trained before, you can use this __pretrained parameters/weights instead of random initialization to solve your problem.__
- It can __help you boost the performance of the NN.__
- The pretrained models might have trained on a large datasets like ImageNet, Ms COCO, or pascal and took a lot of time to learn those parameters/weights with optimized hyperparameters. This can __save you a lot of time.__
- transfer knowledge to your own problem

### Examples
![](transferLearningExamples)
	- Cat classification
		- new class: tigger, misty, neither
		- get a NN ==> remove softmax and final layer ==> add your own softmax and output layer
			- some implementation/libraries will have flags to freeze some of the layers so that you keep the weights that have already been learned and only learn the new softmax or final layers
			- another way is to use the layers that have been learned and treat them like a feature extraction step and save them to disk and then train a very shallow softmax classifier with the saved features
				- you don't need to recompute the numbers in each layer

	- Cat classification with many images
		- new class: tigger, misty, neither
		- same as above but now you freeze first portion of the network, and train the later parts of the network.
			- you have enough data to not only train the softmax classifier but also a couple of network layers as well

	- Cat classification with many many many images
		- new class: tigger, misty, neither
		- You don't freeze any layers but use the layers just as initialization and perform learning in all layers

## Data Augmentation

- If data is increased, your deep NN will perform better. Data augmentation is one of the techniques that deep learning uses to increase the performance of deep NN.
- Some data augmentation methods that are used for computer vision tasks includes:
  - __Mirroring__
  	- only if it preserves what you are trying to classify 
  - __Random cropping__
    - The issue with this technique is that you might take a wrong crop.
    - The solution is to make your crops big enough.
  - in practice these are not used as often because of complexity but they can also be applied to the images
	  - __Rotation__
	  - __Shearing__
	  	- diagonal distortion
	  - __Local warping__
  - __Color shifting__
  	- motivation is that sometimes the lighting is different or filters from cameras are used and by applying these colour shifting can account for these variabilities
    - For example, we add to R, G, and B some distortions that will make the image identified as the same for the human but is different for the computer.
    - In practice the added value are pulled from some probability distribution and these shifts are small
    - Makes your algorithm more robust in changing colors in images. 
    - There are an algorithm which is called ***PCA color augmentation*** that decides the shifts needed in each colour channel automatically

- __Implementing distortions during training:__
![](implementationDistortion)
  - You can use a different CPU thread to make you a distorted mini batches while you are training your NN in parallel
- Data Augmentation has also some hyperparameters. 
	- parameters colour shifting
	- parameters to use for cropping
	- A good place to start is to find an open source data augmentation implementation and then use it or fine tune these hyperparameters.

## State of Computer Vision 

![](dataVsHandEngineering)
- > And I think this is also why that either computer vision has developed rather complex network architectures, is because in the absence of more data the way to get good performance is to spend more time architecting, or fooling around with the network architecture.

- > And someone that is insightful with hand-engineering will get better performance, and is a great contribution to a project to do that hand-engineering when you don't have enough data.
	- > algorithms become even more complex and has even more specialized components.  You see that the algorithms become even more complex and has even more specialized components	

- for little data you have transfer learning
- if you do well on benchmarks than you get more published but people do things that allow you to do well on benchmark but not in an application
	- DO NOT USE IN PRODUCTION:
		- Ensembling
			- train several networks independently and avg their outputs 
			- 1 - 2% better performance
			- running images on 3-15 networks then increase the complexity of the algorithm
		- multi-crop at test time
			- run classifiers on multiple versions of test images and avg the results
			- There is a technique called 10 crops that uses mirrored images and crops square corners to produce a total of 10 crops and obtain the avg of these images.  This increase the amount computational time
- many hand engineering techniques
- Use architectures of networks published in the literature.
- Use open source implementations if possible.
- Use pretrained models and fine-tune on your dataset.