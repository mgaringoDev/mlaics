---
title:  01 - Structuring Machine Learning Projects
author:     Mario Garingo
keywords: deepLearningSpecialization, mlStrat
summary: How to identifying and take the correct ML Strategic decision based on observations of performances and dataset.
category: coursera
type: notes
sidebar: coursera_sidebar
---

## Why ML Strategy

- You have a lot of ideas for how to improve the accuracy of your deep learning system:
  - Collect more data.
  - Collect more diverse training set.
  - Train algorithm longer with gradient descent.
  - Try different optimization algorithm (e.g. Adam).
  - Try bigger network.
  - Try smaller network.
  - Try dropout.
  - Add L2 regularization.
  - Change network architecture (activation functions, # of hidden units, etc.)
- if you choose poorly then it would take a long time in the wrong direction because it takes a long time to run
- __This course will give you some strategies to help analyze your problem to go in a direction that will help you get better results__.

## Orthogonalization

![](orthogonalExamples)
- Some deep learning developers know exactly what hyperparameter to tune in order to try to achieve one effect. This is a process we call orthogonalization.
- In orthogonalization, you have some controls, but each control does a specific task and doesn't affect other controls.
- For a supervised learning system to do well, you usually need to tune the knobs of your system to make sure that four things hold true - __Chain of assumptions in machine learning__:
  1. You'll have to __fit training set well__ on cost function (near human level performance if possible).
     - If it's not achieved you could try:
     	- bigger network
     	- another optimization algorithm (like Adam)
  2. __Fit dev set well__ on cost function.
     - If its not achieved you could try:
     	- regularization
     	- bigger training set
  3. __Fit test set well on cost function__.
     - If its not achieved you could try:
     	- bigger dev. set
  4. __Performs well in real world__.
     - If its not achieved you could try:
     	- change dev. set
     	- change cost function
- __DO NOT USE EARLY STOP__ because it affects both train and dev set performance
![](chainOfAssumption)


## Single number evaluation metric

- Its better and faster to set a single number evaluation metric for your project before you start it.
- Difference between precision and recall (in cat classification example):
  - Suppose we run the classifier on 10 images which are 5 cats and 5 non-cats. The classifier identifies that there are 4 cats, but it identified 1 wrong cat.
  - Confusion matrix:

      |                | Predicted cat  | Predicted non-cat |
      | -------------- | -------------- | ----------------- |
      | Actual cat     | 3              | 2                 |
      | Actual non-cat | 1              | 4                 |
  - **Precision**: percentage of true cats in the recognized result: P = 3/(3 + 1)   	
  - **Recall**: percentage of true recognition cat of the all cat predictions: R = 3/(3 + 2)
  	-	 there is a trade off between precision and recall.
  - **Accuracy**: (3+4)/10
- Using a precision/recall for evaluation is good in a lot of cases, but separately they don't tell you which algorithms is better. 

![](f1Example)

- A better thing is to combine precision and recall in one single (real) number evaluation metric. There a metric called `F1` score, which combines them.  
  - You can think of `F1` score as an average of precision and recall
    `F1 = 2 / ((1/P) + (1/R))`
  - this is the harmonic mean of precision and recall

- A well defined dev set and single real number evalution metric allows you to identify which classifier is better. Speeding up the iteration

## Satisfying and Optimizing metric

- Its hard sometimes to get a single number evaluation metric. 
![](anotherCatExample)
	- accuracy is the optimizing metric
	- running time is the satisficing metric. Meaning that the algorithm is good as long as it meets a specific criteria or threshold.
- So we can solve that by choosing a single optimizing metric and decide that other metrics are satisfying. Ex:
  ```
  Maximize F1                     # optimizing metric
  subject to running time < 100ms # satisficing metric
  ```
- So as a general rule:
  ```
  Maximize 1     # optimizing metric (one optimizing metric)
  subject to N-1 # satisficing metric (N-1 satisficing metrics)
  ```
> summarize if there are multiple things you care about by say there's one as the optimizing metric that you want to do as well as possible on and one or more as satisficing metrics were you'll be satisfied.  These evaluation matrix must be evaluated or calculated on a training set or a development set or maybe on the test set. So one of the things you also need to do is set up training, dev or development, as well as test sets.

## Train/dev/test distributions [I AM HERE]

- Dev and test sets have to come from the same distribution.
- Choose dev set and test set to reflect data you expect to get in the future and consider important to do well on.
- Setting up the dev set, as well as the validation metric is really defining what target you want to aim at.