---
title:  01 - Structuring Machine Learning Projects
author:     Mario Garingo
keywords: deepLearningSpecialization, mlStrat
summary: Learning orthogonalization, how to set up your dev and test sets, human-level performance as a proxy for Bayes error and how to estimate your avoidable bias and variance.
category: coursera
type: notes
sidebar: coursera_sidebar
---

## Why ML Strategy

- You have a lot of ideas for how to improve the accuracy of your deep learning system:
  - Collect more data.
  - Collect more diverse training set.
  - Train algorithm longer with gradient descent.
  - Try different optimization algorithm (e.g. Adam).
  - Try bigger network.
  - Try smaller network.
  - Try dropout.
  - Add L2 regularization.
  - Change network architecture (activation functions, # of hidden units, etc.)
- if you choose poorly then it would take a long time in the wrong direction because it takes a long time to run
- __This course will give you some strategies to help analyze your problem to go in a direction that will help you get better results__.

## Orthogonalization

![](orthogonalExamples)
- Some deep learning developers know exactly what hyperparameter to tune in order to try to achieve one effect. This is a process we call orthogonalization.
- In orthogonalization, you have some controls, but each control does a specific task and doesn't affect other controls.
- For a supervised learning system to do well, you usually need to tune the knobs of your system to make sure that four things hold true - __Chain of assumptions in machine learning__:
  1. You'll have to __fit training set well__ on cost function (near human level performance if possible).
     - If it's not achieved you could try:
     	- bigger network
     	- another optimization algorithm (like Adam)
  2. __Fit dev set well__ on cost function.
     - If its not achieved you could try:
     	- regularization
     	- bigger training set
  3. __Fit test set well on cost function__.
     - If its not achieved you could try:
     	- bigger dev. set
  4. __Performs well in real world__.
     - If its not achieved you could try:
     	- change dev. set
     	- change cost function
- __DO NOT USE EARLY STOP__ because it affects both train and dev set performance
![](chainOfAssumption)


## Single number evaluation metric

- Its better and faster to set a single number evaluation metric for your project before you start it.
- Difference between precision and recall (in cat classification example):
  - Suppose we run the classifier on 10 images which are 5 cats and 5 non-cats. The classifier identifies that there are 4 cats, but it identified 1 wrong cat.
  - Confusion matrix:

      |                | Predicted cat  | Predicted non-cat |
      | -------------- | -------------- | ----------------- |
      | Actual cat     | 3              | 2                 |
      | Actual non-cat | 1              | 4                 |
  - **Precision**: percentage of true cats in the recognized result: P = 3/(3 + 1)   	
  - **Recall**: percentage of true recognition cat of the all cat predictions: R = 3/(3 + 2)
  	-	 there is a trade off between precision and recall.
  - **Accuracy**: (3+4)/10
- Using a precision/recall for evaluation is good in a lot of cases, but separately they don't tell you which algorithms is better. 

![](f1Example)

- A better thing is to combine precision and recall in one single (real) number evaluation metric. There a metric called `F1` score, which combines them.  
  - You can think of `F1` score as an average of precision and recall
    `F1 = 2 / ((1/P) + (1/R))`
  - this is the harmonic mean of precision and recall

- A well defined dev set and single real number evalution metric allows you to identify which classifier is better. Speeding up the iteration

## Satisfying and Optimizing metric

- Its hard sometimes to get a single number evaluation metric. 
![](anotherCatExample)
	- accuracy is the optimizing metric
	- running time is the satisficing metric. Meaning that the algorithm is good as long as it meets a specific criteria or threshold.
- So we can solve that by choosing a single optimizing metric and decide that other metrics are satisfying. Ex:
  ```
  Maximize F1                     # optimizing metric
  subject to running time < 100ms # satisficing metric
  ```
- So as a general rule:
  ```
  Maximize 1     # optimizing metric (one optimizing metric)
  subject to N-1 # satisficing metric (N-1 satisficing metrics)
  ```
> summarize if there are multiple things you care about by say there's one as the optimizing metric that you want to do as well as possible on and one or more as satisficing metrics were you'll be satisfied.  These evaluation matrix must be evaluated or calculated on a training set or a development set or maybe on the test set. So one of the things you also need to do is set up training, dev or development, as well as test sets.

## Train/dev/test distributions 

![](catExample)
- Dev and test sets have to come from the __same distribution__.  
- Choose __dev set and test set to reflect data you expect to get in the future__ and consider important to do well on.
- Setting up the dev set, as well as the validation metric is really defining what target you want to aim at.
- > The way you choose your training set will affect how well you can actually hit that target.

## Size of the dev and test sets

- Recall Test set:
  - The guideline is, __to set your test set to big enough to give high confidence in the overall performance of your system__.
- The old way was valid for a number of examples ~ <100000 
  - An old way of splitting the data was: 
    - 70% training, 30% test 
    - 60% training, 20% dev, 20% test. 
- In the modern deep learning if you have a million or more examples a reasonable split would be 98% training, 1% dev, 1% test. 
  - 1% can be 10 000 samples and that is enough to get a stable algo
- in the past  what sometimes happened was, people were talking about using train test splits but what they were actually doing was iterating on the test set
![](devTest)
  - > if all you care about is having some data that you train on, and having some data to tune to, and you're just going to shake the final system and not worry too much about how it was actually doing, I think it will be healthier and just call the train dev set and acknowledge that you have no test
    - __this is not recommended__ because you still have a bias algo in the end

## When to change dev/test sets and metrics

- Let's take an example. In a cat classification example we have these metric results:
  - In the last example if we choose the best algorithm by metric it would be "A", but if the users decide it will be "B"
![](catExample2)
  - Thus in this case, we want and need to change our metric. 
  - `OldMetric = (1/m) * sum(y_pred[i] != y[i] ,m)`
    - Where m is the number of Dev set items.
    - The old metric treats the porn and non-porn graphics equally.  We want them to not treat them equally.  We want to not mislabel with porn graphics.
      - solution is to add a weight 
  - `NewMetric = (1/sum(w[i])) * sum(w[i] * (y_pred[i] != y[i]) ,m)`
    - where:
       - `w[i] = 1                   if x[i] is not porn`
       - `w[i] = 10                 if x[i] is porn`
  - adding this weighting in the error will increase the porn images and ranking easier.
  - > __when your evaluation metric is no longer correctly rank ordering preferences between algorithms__, in this case is mispredicting that Algorithm A is a better algorithm, then that's a sign that __you should change your evaluation metric or perhaps your development set or test set__

- This is actually an example of an __orthogonalization problem__ where you should take a machine learning problem and break it into distinct steps: 

  1. Figure out how to define a metric that captures what you want to do 
    - place the target. 
  2. Worry about how to actually do well on this metric 
    - how to aim/shoot accurately at the target.

- another example cat:
  - dev/test are high quality images while real app uses blurry uploaded images and algo B works better than algo A then you should change the error metric and also the dev/test set to better capture this blurry uploaded images
- __Conclusion: if doing well on your metric + dev/test set doesn't correspond to doing well in your application, change your metric and/or dev/test set.__

- > what I recommend against for the most teams is to run for too long without any evaluation metric and dev set up because that can slow down the efficiency of what your team can iterate and improve your algorithm


## Why human-level performance?

- > We compare to human-level performance because of two main reasons:
  > 1. Because of advances in deep learning, machine learning algorithms are suddenly working much better and so it has become much more feasible in a lot of application areas for machine learning algorithms to actually become competitive with human-level performance. 
  > 2. It turns out that the workflow of designing and building a machine learning system is much more efficient when you're trying to do something that humans can also do.
- After an algorithm reaches the human level performance the progress and accuracy slow down.
    ![01- Why human-level performance](humanLevelPerformance)
- You won't surpass an error that's called __Bayes optimal error__.
- There isn't much error range between human-level error and Bayes optimal error.
- Humans are quite good at a lot of tasks. So __as long as Machine learning is worse than humans, you can__:
  - Get labeled data from humans.
  - Gain insight from manual error analysis: why did a person get it right?
  - Better analysis of bias/variance.


## Avoidable bias

![](catBias)
- Suppose that the cat classification algorithm gives these results:
  - In the left example, because the human level error is 1% then we have to focus on the **bias**.
  - In the right example, because the human level error is 7.5% then we have to focus on the **variance**.
  - __The human-level error as a proxy (estimate) for Bayes optimal error. Bayes optimal error is always less (better), but human-level in most cases is not far from it.__
    - You can't do better than Bayes error unless you are overfitting.
  - `Avoidable bias = Training error - Human (Bayes) error`
    - the difference between Bayes and training error
  - `Variance = Dev error - Training error`
    - the difference between dev and training error

### Understanding human-level performance

- defining `human level performance`
  - > show that you can surpass a single human and therefore argue for deploying your system in some context, maybe this is the appropriate definition
  - > goal is to proxy for Bayes error then the best needs to be desirable one
- When choosing human-level performance, it has to be chosen in the terms of what you want to achieve with the system.
- You might have multiple human-level performances based on the human experience. Then you choose the human-level performance (proxy for Bayes error) that is more suitable for the system you're trying to build.
- Improving deep learning algorithms is harder once you reach a human-level performance.

![](errorAnalysisExample)
- Summary of bias/variance with human-level performance:
  1. human-level error (proxy for Bayes error)
     - Calculate `avoidable bias = training error - human-level error`
     - If **avoidable bias** difference is the bigger, then it's *bias* problem and you should use a strategy for **bias** resolving.
  2. training error
     - Calculate `variance = dev error - training error`
     - If **variance** difference is bigger, then you should use a strategy for **variance** resolving.
  3. Dev error
- So having an estimate of human-level performance gives you an estimate of Bayes error. And this __allows you to more quickly make decisions as to whether you should focus on trying to reduce a bias or trying to reduce the variance of your algorithm.__
![](summaryHumanLevelError)
> So to recap, having an estimate of human-level performance gives you an estimate of Bayes error.  And this allows you to more quickly make decisions as to whether you should focus on trying to reduce a bias or trying to reduce the variance of your algorithm.
- These techniques will tend to work well until you surpass human-level performance, whereupon you might no longer have a good estimate of Bayes error that still helps you make this decision really clearly. 


## Surpassing human-level performance

- In some problems, deep learning has surpassed human-level performance. Like:
  - Online advertising.
  - Product recommendation.
  - logistics (predicting transit time)
  - Loan approval.
- The last examples are not natural perception task, rather learning on structural data. Humans are far better in natural perception tasks like computer vision and speech recognition.
- computers examine more data than a human computers can surpass human level performance
- It's harder for machines to surpass human-level performance in natural perception task. But there are already some systems that achieved it.


## Improving your model performance

- The two fundamental assumptions of supervised learning:
  1. You can fit the training set pretty well. 
    - This is roughly saying that you can achieve low **avoidable bias**. 
  2. The training set performance generalizes pretty well to the dev/test set. 
    - This is roughly saying that **variance** is not too bad.
  - you can address this in an **orthogonal** manner
- To improve your deep learning supervised system follow these guidelines:
  1. Look at the difference between human level error and the training error - **avoidable bias**.
  2. Look at the difference between the dev/test set and training set error - **Variance**.
  3. If **avoidable bias** is large you have these options:
    - Train bigger model.
    - Train longer/better optimization algorithm (like Momentum, RMSprop, Adam).
    - Find better NN architecture/hyperparameters search.
      - changing the number of nodes and hidden layer
      - changing the activation function
  4. If **variance** is large you have these options:
    - being better at generalization
    - Get more training data.
    - Regularization (L2, Dropout, data augmentation).
    - Find better NN architecture/hyperparameters search.
      - changing the number of nodes and hidden layer
      - changing the activation function









