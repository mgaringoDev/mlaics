@article{wolfe1990american,
	title={The American College of Rheumatology 1990 criteria for the classification of fibromyalgia},
	author={Wolfe, Frederick and Smythe, Hugh A and Yunus, Muhammad B and Bennett, Robert M and Bombardier, Claire and Goldenberg, Don L and Tugwell, Peter and Campbell, Stephen M and Abeles, Micha and Clark, Patricia and others},
	journal={Arthritis \& Rheumatism: Official Journal of the American College of Rheumatology},
	volume={33},
	number={2},
	pages={160--172},
	year={1990},
	publisher={Wiley Online Library}
}

@article{someTest,
    author = {Mateescu, Radu and Poizat, Pascal and Salaun, Gwen},
    year = {2012},
    title = {{Adaptation of Service Protocols using Process Algebra and On-the-Fly Reduction Techniques}},
    journal = {IEEE Transactions on Software Engineering},
    pages = {755--777},
    volume = {38},
    number = {4},
    DOI = {10.1109/TSE.2011.62},
    HAL_ID = {hal-00717252},
    PDF = {https://hal.inria.fr/hal-00717252/document},
}


@article{sadowski_notes_2016,
	title = {Notes on backpropagation},
	journal = {homepage: https://www. ics. uci. edu/pjsadows/notes. pdf (online)},
	author = {Sadowski, Peter},
	year = {2016},
	extra = {1Q\_j9\_JUGp8Mab7IdO-SOFgRkrMeaGuuA},
	file = {Sadowski_2016_Notes on backpropagation.pdf:C\:\\Users\\HP\\Zotero\\storage\\EWHCBMD6\\Sadowski_2016_Notes on backpropagation.pdf:application/pdf}
}

@misc{noauthor_backpropagation_nodate,
	title = {Backpropagation {\textemdash} {ML} {Glossary} documentation},
	url = {https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html#applying-the-chain-rule},
	urldate = {2020-05-27},
	extra =  {1rg6GxkZL26EFy2eIMAbu7sVyR5CIiR8M},
	file = {Backpropagation {\textemdash} ML Glossary documentation:C\:\\Users\\HP\\Zotero\\storage\\YP3UZ36X\\backpropagation.html:text/html;Backpropagation {\textemdash} ML Glossary documentation.pdf:C\:\\Users\\HP\\Zotero\\storage\\4SPZNKNQ\\Backpropagation {\textemdash} ML Glossary documentation.pdf:application/pdf}
}

@misc{noauthor_backpropagation_nodate-1,
	title = {backpropagation - {Neural} network softmax activation},
	url = {https://stats.stackexchange.com/questions/273465/neural-network-softmax-activation},
	urldate = {2020-05-27},
	journal = {Cross Validated},
	extra =  {1XCwEy7EzPaPAxlWzgL63y6BsAVmzMQm\_},
	file = {Snapshot:C\:\\Users\\HP\\Zotero\\storage\\D7LE7DKT\\neural-network-softmax-activation.html:text/html;backpropagation - Neural network softmax activation.pdf:C\:\\Users\\HP\\Zotero\\storage\\TUBXHWUC\\backpropagation - Neural network softmax activation.pdf:application/pdf}
}

@misc{noauthor_machine_nodate,
	title = {machine learning - {Backpropagation} - softmax derivative},
	url = {https://datascience.stackexchange.com/questions/32949/backpropagation-softmax-derivative},
	urldate = {2020-05-27},
	journal = {Data Science Stack Exchange},
	extra =  {1SKYJIVJCPQuzICt3Nqc3VDS1kW-AprL0},
	file = {Snapshot:C\:\\Users\\HP\\Zotero\\storage\\3XHZIAV9\\backpropagation-softmax-derivative.html:text/html;machine learning - Backpropagation - softmax derivative.pdf:C\:\\Users\\HP\\Zotero\\storage\\N5L6F2HJ\\machine learning - Backpropagation - softmax derivative.pdf:application/pdf}
}

@misc{dahal_classification_2017,
	title = {Classification and {Loss} {Evaluation} - {Softmax} and {Cross} {Entropy} {Loss}},
	url = {http://deepnotes.io/softmax-crossentropy},
	abstract = {Lets dig a little deep into how we convert the output of our CNN into probability - Softmax; and the loss measure to guide our optimization - Cross Entropy.},
	language = {en-us},
	urldate = {2020-05-27},
	journal = {DeepNotes},
	author = {Dahal, Paras},
	month = may,
	year = {2017},
	extra =  {1LXycYw5rOkGLl\_DAdUhfPh44pPJQiHXE},
	file = {Snapshot:C\:\\Users\\HP\\Zotero\\storage\\IX8MVVDK\\softmax-crossentropy.html:text/html;Dahal_2017_Classification and Loss Evaluation - Softmax and Cross Entropy Loss.pdf:C\:\\Users\\HP\\Zotero\\storage\\TML7VEX6\\Dahal_2017_Classification and Loss Evaluation - Softmax and Cross Entropy Loss.pdf:application/pdf}
}

@misc{noauthor_calculus_nodate,
	title = {calculus - {Derivative} of {Softmax} without cross entropy},
	url = {https://math.stackexchange.com/questions/2843505/derivative-of-softmax-without-cross-entropy},
	urldate = {2020-05-27},
	journal = {Mathematics Stack Exchange},
	extra =  {1de2CZnVY3D9cF-4C0pPQy64OoLNJsWiZ},
	file = {Snapshot:C\:\\Users\\HP\\Zotero\\storage\\P3ICSXAM\\derivative-of-softmax-without-cross-entropy.html:text/html;calculus - Derivative of Softmax without cross entropy.pdf:C\:\\Users\\HP\\Zotero\\storage\\9PEXRSNS\\calculus - Derivative of Softmax without cross entropy.pdf:application/pdf}
}

@misc{noauthor_softmax_nodate,
	title = {The {Softmax} function and its derivative - {Eli} {Bendersky}'s website},
	url = {https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/},
	urldate = {2020-05-27},
	extra =  {1ZDP13K0OoQNKoa9TxO3RiYJz8JyKpBls},
	file = {The Softmax function and its derivative - Eli Bendersky's website:C\:\\Users\\HP\\Zotero\\storage\\BHXP2Y7B\\the-softmax-function-and-its-derivative.html:text/html;The Softmax function and its derivative - Eli Bendersky's website.pdf:C\:\\Users\\HP\\Zotero\\storage\\F2JZJY8K\\The Softmax function and its derivative - Eli Bendersky's website.pdf:application/pdf}
}

@misc{britz_implementing_2015,
	title = {Implementing a {Neural} {Network} from {Scratch} in {Python} {\textendash} {An} {Introduction}},
	url = {http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/},
	abstract = {Get the code: To follow along, all the code is also available as an iPython notebook on Github. In this post we will implement a simple 3-layer neural network from scratch. We won{\textquoteright}t derive al{\textellipsis}},
	language = {en-US},
	urldate = {2020-05-27},
	journal = {WildML},
	author = {Britz, Denny},
	month = sep,
	year = {2015},
	extra =  {1Qwgdl8bBJzhU1Xe9bYtQUDR2TbIkI6XU},
	file = {Snapshot:C\:\\Users\\HP\\Zotero\\storage\\92CN6WHX\\implementing-a-neural-network-from-scratch.html:text/html;Britz_2015_Implementing a Neural Network from Scratch in Python {\textendash} An Introduction.pdf:C\:\\Users\\HP\\Zotero\\storage\\5EKGTDQW\\Britz_2015_Implementing a Neural Network from Scratch in Python {\textendash} An Introduction.pdf:application/pdf}
}

@misc{noauthor_derivative_nodate,
	title = {derivative - {Backpropagation} with {Softmax} / {Cross} {Entropy}},
	url = {https://stats.stackexchange.com/questions/235528/backpropagation-with-softmax-cross-entropy},
	urldate = {2020-05-27},
	journal = {Cross Validated},
	extra =  {10Ff5JuZq-6FjxY-VGq\_wd0b0nf8K9BCx},
	file = {Snapshot:C\:\\Users\\HP\\Zotero\\storage\\YGNYTJAJ\\backpropagation-with-softmax-cross-entropy.html:text/html;derivative - Backpropagation with Softmax - Cross Entropy.pdf:C\:\\Users\\HP\\Zotero\\storage\\32VY7K6B\\derivative - Backpropagation with Softmax - Cross Entropy.pdf:application/pdf}
}

@misc{noauthor_linear_nodate,
	title = {linear algebra - {Derivative} of {Softmax} loss function},
	url = {https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function},
	urldate = {2020-05-27},
	journal = {Mathematics Stack Exchange},
	extra =  {13OXGFKsav3S-2Retup0szV3ddpmWW1N-},
	file = {Snapshot:C\:\\Users\\HP\\Zotero\\storage\\JPMJCDZV\\derivative-of-softmax-loss-function.html:text/html;linear algebra - Derivative of Softmax loss function.pdf:C\:\\Users\\HP\\Zotero\\storage\\Z5FVSME5\\linear algebra - Derivative of Softmax loss function.pdf:application/pdf}
}

@misc{kim__how_2018,
	title = {How to implement the {Softmax} derivative independently from any loss function?},
	url = {https://medium.com/@aerinykim/how-to-implement-the-softmax-derivative-independently-from-any-loss-function-ae6d44363a9d},
	abstract = {Mathematically, the derivative of Softmax $\sigma$(j) with respect to the logit Zi (for example, Wi*X) is},
	language = {en},
	urldate = {2020-05-27},
	journal = {Medium},
	author = {Kim ??, Aerin},
	month = oct,
	year = {2018},
	extra =  {1m-shVrOusOBXr7YF1wEL7iCk\_5FdecWl},
	file = {Snapshot:C\:\\Users\\HP\\Zotero\\storage\\6UA3IYKY\\how-to-implement-the-softmax-derivative-independently-from-any-loss-function-ae6d44363a9d.html:text/html;Kim _2018_How to implement the Softmax derivative independently from any loss function.pdf:C\:\\Users\\HP\\Zotero\\storage\\PCJBBRMY\\Kim _2018_How to implement the Softmax derivative independently from any loss function.pdf:application/pdf}
}